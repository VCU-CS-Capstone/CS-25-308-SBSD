# SBSD RAG Chatbot (Dockerized)

This project is a Retrieval-Augmented Generation (RAG) chatbot built with:

- ğŸ§  [LangChain](https://www.langchain.com/) + [Ollama](https://ollama.com/) for LLM and embeddings
- âš™ï¸ Flask backend (Python)
- ğŸ’¬ React frontend (Next.js + React Chatbotify)
- ğŸ³ Fully Dockerized for ease of deployment

---

## ğŸ› ï¸ Prerequisites

Make sure the following are installed:

- [Docker](https://www.docker.com/products/docker-desktop)
- [Git](https://git-scm.com/downloads)
- [Ollama](https://ollama.com/) (for local LLM + embedding models)

---

## ğŸ“¥ Step 1: Clone the Repository

```bash
git clone https://github.com/your-username/your-repo-name.git
cd CS-25-308-SBSD/src
```

---

## ğŸ§  Step 2: Set Up Ollama

1. **Install Ollama**

2. **Pull required models**

```bash
ollama pull qwen2.5:latest
ollama pull nomic-embed-text:latest
```

3. **Run the Ollama server**

```bash
ollama serve &
```

> ğŸ§  Ollama runs at http://localhost:11434

---

## ğŸ³ Step 3: Run the Dockerized App

From the root of the project:

```bash
docker compose build --no-cache
docker compose up
```

This will:
- Start the **Flask backend** at http://localhost:5000
- Start the **React frontend** at http://localhost:3000

---

## ğŸ§ª Step 4: Interact with the Chatbot

Open your browser and go to:

```
http://localhost:3000
```

Type your question and receive answers powered by Ollama's local LLM.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ backend/             # Python backend (Flask + LangChain)
â”‚   â”œâ”€â”€ frontend/            # React/Next.js frontend
â”‚   â”œâ”€â”€ data/                # PDF / FAQ documents
â”‚   â”œâ”€â”€ Dockerfile.backend   # Dockerfile for backend
â”‚   â”œâ”€â”€ Dockerfile.frontend  # Dockerfile for frontend
â”‚   â”œâ”€â”€ requirements-docker.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

